{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormLayer(Layer):\n",
    "    def __init__(self, incoming, axes='auto', epsilon=1e-4, alpha=0.1,\n",
    "                 beta=init.Constant(0), gamma=init.Constant(1),\n",
    "                 mean=init.Constant(0), inv_std=init.Constant(1), **kwargs):\n",
    "        super(LayerNormLayer, self).__init__(incoming, **kwargs)\n",
    "\n",
    "        if axes == 'auto':\n",
    "            # default: normalize over all\n",
    "            axes = tuple(range(0, len(self.input_shape)))\n",
    "        elif isinstance(axes, int):\n",
    "            axes = (axes,)\n",
    "        self.axes = axes\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "        if beta is None:\n",
    "            self.beta = None\n",
    "        else:\n",
    "            self.beta = self.add_param(beta, 'beta',\n",
    "                                       trainable=True, regularizable=False)\n",
    "        if gamma is None:\n",
    "            self.gamma = None\n",
    "        else:\n",
    "            self.gamma = self.add_param(gamma, 'gamma',\n",
    "                                        trainable=True, regularizable=True)\n",
    "        self.mean = self.add_param(mean, 'mean',\n",
    "                                   trainable=False, regularizable=False)\n",
    "        self.inv_std = self.add_param(inv_std, 'inv_std',\n",
    "                                      trainable=False, regularizable=False)\n",
    "\n",
    "    def get_output_for(self, input, deterministic=False, **kwargs):\n",
    "        input_mean = input.mean(self.axes)\n",
    "        input_inv_std = T.inv(T.sqrt(input.var(self.axes) + self.epsilon))\n",
    "\n",
    "        mean = input_mean\n",
    "        inv_std = input_inv_std\n",
    "\n",
    "        # prepare dimshuffle pattern inserting broadcastable axes as needed\n",
    "        param_axes = iter(range(input.ndim - len(self.axes)))\n",
    "        pattern = ['x' if input_axis in self.axes\n",
    "                   else next(param_axes)\n",
    "                   for input_axis in range(input.ndim)]\n",
    "\n",
    "        # apply dimshuffle pattern to all parameters\n",
    "        beta = 0 if self.beta is None else self.beta.dimshuffle(pattern)\n",
    "        gamma = 1 if self.gamma is None else self.gamma.dimshuffle(pattern)\n",
    "        mean = mean.dimshuffle(pattern)\n",
    "        inv_std = inv_std.dimshuffle(pattern)\n",
    "\n",
    "        # normalize\n",
    "        normalized = (input - mean) * (gamma * inv_std) + beta\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormLayer(Layer):\n",
    "    def __init__(self, incoming, axes='auto', epsilon=1e-4, alpha=0.1,\n",
    "                 beta=init.Constant(0), gamma=init.Constant(1),\n",
    "                 mean=init.Constant(0), inv_std=init.Constant(1), **kwargs):\n",
    "        super(LayerNormLayer, self).__init__(incoming, **kwargs)\n",
    "\n",
    "        if axes == 'auto':\n",
    "            # default: normalize over all\n",
    "            axes = tuple(range(0, len(self.input_shape)))\n",
    "        elif isinstance(axes, int):\n",
    "            axes = (axes,)\n",
    "        self.axes = axes\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "        if beta is None:\n",
    "            self.beta = None\n",
    "        else:\n",
    "            self.beta = self.add_param(beta, 'beta',\n",
    "                                       trainable=True, regularizable=False)\n",
    "        if gamma is None:\n",
    "            self.gamma = None\n",
    "        else:\n",
    "            self.gamma = self.add_param(gamma, 'gamma',\n",
    "                                        trainable=True, regularizable=True)\n",
    "        self.mean = self.add_param(mean, 'mean',\n",
    "                                   trainable=False, regularizable=False)\n",
    "        self.inv_std = self.add_param(inv_std, 'inv_std',\n",
    "                                      trainable=False, regularizable=False)\n",
    "\n",
    "    def get_output_for(self, input, deterministic=False, **kwargs):\n",
    "        input_mean = input.mean(self.axes)\n",
    "        input_inv_std = T.inv(T.sqrt(input.var(self.axes) + self.epsilon))\n",
    "\n",
    "        mean = input_mean\n",
    "        inv_std = input_inv_std\n",
    "\n",
    "        # prepare dimshuffle pattern inserting broadcastable axes as needed\n",
    "        param_axes = iter(range(input.ndim - len(self.axes)))\n",
    "        pattern = ['x' if input_axis in self.axes\n",
    "                   else next(param_axes)\n",
    "                   for input_axis in range(input.ndim)]\n",
    "\n",
    "        # apply dimshuffle pattern to all parameters\n",
    "        beta = 0 if self.beta is None else self.beta.dimshuffle(pattern)\n",
    "        gamma = 1 if self.gamma is None else self.gamma.dimshuffle(pattern)\n",
    "        mean = mean.dimshuffle(pattern)\n",
    "        inv_std = inv_std.dimshuffle(pattern)\n",
    "\n",
    "        # normalize\n",
    "        normalized = (input - mean) * (gamma * inv_std) + beta\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LNGRULayer(MergeLayer):\n",
    "    r\"\"\"\n",
    "    .. math ::\n",
    "        r_t = \\sigma_r(LN(x_t, W_{xr}; \\gamma_{xr}, \\beta_{xr}) + LN(h_{t - 1}, W_{hr}; \\gamma_{xr}, \\beta_{xr}) + b_r)\\\\ \\\n",
    "        u_t = \\sigma_u(LN(x_t, W_{xu}; \\gamma_{xu}, \\beta_{xu}) + LN(h_{t - 1}, W_{hu}; \\gamma_{xu}, \\beta_{xu})+ b_u)\\\\ \\\n",
    "        c_t = \\sigma_c(LN(x_t, W_{xc}; \\gamma_{xc}, \\beta_{xc}) + r_t \\odot (LN(h_{t - 1}, W_{hc}); \\gamma_{xc}, \\beta_{xc}) + b_c)\\\\ \\\n",
    "        h_t = (1 - u_t) \\odot h_{t - 1} + u_t \\odot c_t \\\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    .. math::\n",
    "        LN(z;\\alpha, \\beta) = \\frac{(z-\\mu)}{\\sigma} \\odot \\alpha + \\beta\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, incoming, num_units,\n",
    "                 resetgate=Gate(W_cell=None),\n",
    "                 updategate=Gate(W_cell=None),\n",
    "                 hidden_update=Gate(W_cell=None,\n",
    "                                    nonlinearity=nonlinearities.tanh),\n",
    "                 hid_init=init.Constant(0.),\n",
    "                 backwards=False,\n",
    "                 learn_init=False,\n",
    "                 gradient_steps=-1,\n",
    "                 grad_clipping=0,\n",
    "                 unroll_scan=False,\n",
    "                 precompute_input=True,\n",
    "                 mask_input=None,\n",
    "                 only_return_final=False,\n",
    "                 alpha_init=init.Constant(1.0),\n",
    "                 beta_init=init.Constant(0.0),\n",
    "                 normalize_hidden_update=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        # This layer inherits from a MergeLayer, because it can have three\n",
    "        # inputs - the layer input, the mask and the initial hidden state.  We\n",
    "        # will just provide the layer input as incomings, unless a mask input\n",
    "        # or initial hidden state was provided.\n",
    "        incomings = [incoming]\n",
    "        self.mask_incoming_index = -1\n",
    "        self.hid_init_incoming_index = -1\n",
    "        if mask_input is not None:\n",
    "            incomings.append(mask_input)\n",
    "            self.mask_incoming_index = len(incomings)-1\n",
    "        if isinstance(hid_init, Layer):\n",
    "            incomings.append(hid_init)\n",
    "            self.hid_init_incoming_index = len(incomings)-1\n",
    "\n",
    "        # Initialize parent layer\n",
    "        super(LNGRULayer, self).__init__(incomings, **kwargs)\n",
    "\n",
    "        # # If the provided nonlinearity is None, make it linear\n",
    "        # if nonlinearity is None:\n",
    "        #     self.nonlinearity = nonlinearities.identity\n",
    "        # else:\n",
    "        #     self.nonlinearity = nonlinearity\n",
    "\n",
    "        self.learn_init = learn_init\n",
    "        self.num_units = num_units\n",
    "        self.grad_clipping = grad_clipping\n",
    "        self.backwards = backwards\n",
    "        self.gradient_steps = gradient_steps\n",
    "        self.unroll_scan = unroll_scan\n",
    "        self.precompute_input = precompute_input\n",
    "        self.only_return_final = only_return_final\n",
    "\n",
    "        self.alpha_init = alpha_init\n",
    "        self.beta_init = beta_init\n",
    "        self.normalize_hidden_update = normalize_hidden_update\n",
    "        self._eps = 1e-5\n",
    "\n",
    "        if unroll_scan and gradient_steps != -1:\n",
    "            raise ValueError(\n",
    "                \"Gradient steps must be -1 when unroll_scan is true.\")\n",
    "\n",
    "        # Retrieve the dimensionality of the incoming layer\n",
    "        input_shape = self.input_shapes[0]\n",
    "\n",
    "        if unroll_scan and input_shape[1] is None:\n",
    "            raise ValueError(\"Input sequence length cannot be specified as \"\n",
    "                             \"None when unroll_scan is True\")\n",
    "\n",
    "        # Input dimensionality is the output dimensionality of the input layer\n",
    "        num_inputs = np.prod(input_shape[2:])\n",
    "\n",
    "        def add_gate_params(gate, gate_name):\n",
    "            \"\"\" Convenience function for adding layer parameters from a Gate\n",
    "            instance. \"\"\"\n",
    "            return (self.add_param(gate.W_in, (num_inputs, num_units),\n",
    "                                   name=\"W_in_to_{}\".format(gate_name)),\n",
    "                    self.add_param(gate.W_hid, (num_units, num_units),\n",
    "                                   name=\"W_hid_to_{}\".format(gate_name)),\n",
    "                    self.add_param(gate.b, (num_units,),\n",
    "                                   name=\"b_{}\".format(gate_name),\n",
    "                                   regularizable=False),\n",
    "                    self.add_param(self.alpha_init, (num_units,),\n",
    "                                   name=\"alpha_in_to_{}\".format(gate_name)),\n",
    "                    self.add_param(self.beta_init, (num_units,),\n",
    "                                   name=\"beta_in_to_{}\".format(gate_name),\n",
    "                                   regularizable=False),\n",
    "                    self.add_param(self.alpha_init, (num_units,),\n",
    "                                   name=\"alpha_hid_to_{}\".format(gate_name)),\n",
    "                    self.add_param(self.beta_init, (num_units,),\n",
    "                                   name=\"beta_hid_to_{}\".format(gate_name),\n",
    "                                   regularizable=False),\n",
    "                    gate.nonlinearity)\n",
    "\n",
    "        # Add in all parameters from gates\n",
    "        (self.W_in_to_updategate, self.W_hid_to_updategate, self.b_updategate,\n",
    "         self.alpha_in_to_updategate, self.beta_in_to_updategate,\n",
    "         self.alpha_hid_to_updategate, self.beta_hid_to_updategate,\n",
    "         self.nonlinearity_updategate) = add_gate_params(updategate, 'updategate')\n",
    "\n",
    "        (self.W_in_to_resetgate, self.W_hid_to_resetgate, self.b_resetgate,\n",
    "         self.alpha_in_to_resetgate, self.beta_in_to_resetgate,\n",
    "         self.alpha_hid_to_resetgate, self.beta_hid_to_resetgate,\n",
    "         self.nonlinearity_resetgate) = add_gate_params(resetgate, 'resetgate')\n",
    "\n",
    "        (self.W_in_to_hidden_update, self.W_hid_to_hidden_update, self.b_hidden_update, \n",
    "         self.alpha_in_to_hidden_update, self.beta_in_to_hidden_update,\n",
    "         self.alpha_hid_to_hidden_update, self.beta_hid_to_hidden_update,\n",
    "         self.nonlinearity_hidden_update) = add_gate_params(hidden_update, 'hidden_update')\n",
    "\n",
    "        # Initialize hidden state\n",
    "        if isinstance(hid_init, Layer):\n",
    "            self.hid_init = hid_init\n",
    "        else:\n",
    "            self.hid_init = self.add_param(\n",
    "                hid_init, (1, self.num_units), name=\"hid_init\",\n",
    "                trainable=learn_init, regularizable=False)\n",
    "\n",
    "        # parameters for Layer Normalization of the cell gate\n",
    "        if self.normalize_hidden_update:\n",
    "            self.alpha_hidden_update = self.add_param(\n",
    "                self.alpha_init, (num_units, ),\n",
    "                name=\"alpha_hidden_update\")\n",
    "            self.beta_hidden_update = self.add_param(\n",
    "                self.beta_init, (num_units, ),\n",
    "                name=\"beta_hidden_update\", regularizable=False)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        # The shape of the input to this layer will be the first element\n",
    "        # of input_shapes, whether or not a mask input is being used.\n",
    "        input_shape = input_shapes[0]\n",
    "        # When only_return_final is true, the second (sequence step) dimension\n",
    "        # will be flattened\n",
    "        if self.only_return_final:\n",
    "            return input_shape[0], self.num_units\n",
    "        # Otherwise, the shape will be (n_batch, n_steps, num_units)\n",
    "        else:\n",
    "            return input_shape[0], input_shape[1], self.num_units\n",
    "\n",
    "    # Layer Normalization\n",
    "    def __ln__(self, z, alpha, beta):\n",
    "        output = (z - z.mean(-1, keepdims=True)) / T.sqrt(z.var(-1, keepdims=True) + self._eps)\n",
    "        output = alpha * output + beta\n",
    "        return output\n",
    "\n",
    "\n",
    "    def __gru_fun__(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Retrieve the layer input\n",
    "        input = inputs[0]\n",
    "        # Retrieve the mask when it is supplied\n",
    "        mask = None\n",
    "        hid_init = None\n",
    "        if self.mask_incoming_index > 0:\n",
    "            mask = inputs[self.mask_incoming_index]\n",
    "        if self.hid_init_incoming_index > 0:\n",
    "            hid_init = inputs[self.hid_init_incoming_index]\n",
    "\n",
    "        # Treat all dimensions after the second as flattened feature dimensions\n",
    "        if input.ndim > 3:\n",
    "            input = T.flatten(input, 3)\n",
    "\n",
    "        # Because scan iterates over the first dimension we dimshuffle to\n",
    "        # (n_time_steps, n_batch, n_features)\n",
    "        input = input.dimshuffle(1, 0, 2)\n",
    "        seq_len, num_batch, _ = input.shape\n",
    "\n",
    "        # Stack input weight matrices into a (num_inputs, 3*num_units)\n",
    "        # matrix, which speeds up computation\n",
    "        W_in_stacked = T.concatenate(\n",
    "            [self.W_in_to_resetgate, self.W_in_to_updategate,\n",
    "             self.W_in_to_hidden_update], axis=1)\n",
    "\n",
    "        # Stack alphas for input into a (3*num_units) vector\n",
    "        alpha_in_stacked = T.concatenate(\n",
    "            [self.alpha_in_to_resetgate, self.alpha_in_to_updategate,\n",
    "             self.alpha_in_to_hidden_update], axis=0)\n",
    "\n",
    "        # Stack betas for input into a (3*num_units) vector\n",
    "        beta_in_stacked = T.concatenate(\n",
    "            [self.beta_in_to_resetgate, self.beta_in_to_updategate,\n",
    "             self.beta_in_to_hidden_update], axis=0)\n",
    "\n",
    "        # Same for hidden weight matrices\n",
    "        W_hid_stacked = T.concatenate(\n",
    "            [self.W_hid_to_resetgate, self.W_hid_to_updategate,\n",
    "             self.W_hid_to_hidden_update], axis=1)\n",
    "\n",
    "        # Stack alphas for hidden into a (3*num_units) vector\n",
    "        alpha_hid_stacked = T.concatenate(\n",
    "            [self.alpha_hid_to_resetgate, self.alpha_hid_to_updategate,\n",
    "             self.alpha_hid_to_hidden_update], axis=0)\n",
    "\n",
    "        # Stack betas for hidden into a (3*num_units) vector\n",
    "        beta_hid_stacked = T.concatenate(\n",
    "            [self.beta_hid_to_resetgate, self.beta_hid_to_updategate,\n",
    "             self.beta_hid_to_hidden_update], axis=0)\n",
    "\n",
    "        # Stack gate biases into a (3*num_units) vector\n",
    "        b_stacked = T.concatenate(\n",
    "            [self.b_resetgate, self.b_updategate,\n",
    "             self.b_hidden_update], axis=0)\n",
    "\n",
    "        if self.precompute_input:\n",
    "            # precompute_input inputs*W. W_in is (n_features, 3*num_units).\n",
    "            # input is then (n_batch, n_time_steps, 3*num_units).\n",
    "            big_ones = T.ones((seq_len, num_batch, 1))\n",
    "            input = T.dot(input, W_in_stacked)\n",
    "            input = self.__ln__(input,\n",
    "                                T.dot(big_ones, alpha_in_stacked.dimshuffle('x', 0)),\n",
    "                                beta_in_stacked) + b_stacked\n",
    "        \n",
    "        ones = T.ones((num_batch, 1))\n",
    "        # At each call to scan, input_n will be (n_time_steps, 3*num_units).\n",
    "        # We define a slicing function that extract the input to each GRU gate\n",
    "        def slice_w(x, n):\n",
    "            return x[:, n*self.num_units:(n+1)*self.num_units]\n",
    "\n",
    "        # Create single recurrent computation step function\n",
    "        # input__n is the n'th vector of the input\n",
    "        def step(input_n, hid_previous, *args):\n",
    "            if not self.precompute_input:\n",
    "                # Compute W_{xr}x_t + b_r, W_{xu}x_t + b_u, and W_{xc}x_t + b_c\n",
    "                input_n = T.dot(input_n, W_in_stacked)\n",
    "                input_n = self.__ln__(input_n,\n",
    "                                      T.dot(ones, alpha_in_stacked.dimshuffle('x', 0)),\n",
    "                                      beta_in_stacked) + b_stacked\n",
    "\n",
    "            # Compute W_{hr} h_{t - 1}, W_{hu} h_{t - 1}, and W_{hc} h_{t - 1}\n",
    "            hid_input = T.dot(hid_previous, W_hid_stacked)\n",
    "            hid_input = self.__ln__(hid_input,\n",
    "                                    T.dot(ones, alpha_hid_stacked.dimshuffle('x', 0)),\n",
    "                                    beta_hid_stacked)\n",
    "\n",
    "            # Reset and update gates\n",
    "            resetgate = slice_w(hid_input, 0) + slice_w(input_n, 0)\n",
    "            updategate = slice_w(hid_input, 1) + slice_w(input_n, 1)\n",
    "            if self.grad_clipping:\n",
    "                resetgate = theano.gradient.grad_clip(\n",
    "                    resetgate, -self.grad_clipping, self.grad_clipping)\n",
    "                updategate = theano.gradient.grad_clip(\n",
    "                    updategate, -self.grad_clipping, self.grad_clipping)\n",
    "\n",
    "            resetgate = self.nonlinearity_resetgate(resetgate)\n",
    "            updategate = self.nonlinearity_updategate(updategate)\n",
    "\n",
    "            # Compute W_{xc}x_t + r_t \\odot (W_{hc} h_{t - 1})\n",
    "            hidden_update_in = slice_w(input_n, 2)\n",
    "            hidden_update_hid = slice_w(hid_input, 2)\n",
    "            hidden_update = hidden_update_in + resetgate*hidden_update_hid\n",
    "            if self.grad_clipping:\n",
    "                hidden_update = theano.gradient.grad_clip(\n",
    "                    hidden_update, -self.grad_clipping, self.grad_clipping)\n",
    "            hidden_update = self.nonlinearity_hidden_update(hidden_update)\n",
    "\n",
    "            if self.normalize_hidden_update:\n",
    "                hidden_update = self.__ln__(hidden_update,\n",
    "                                   T.dot(ones, self.alpha_hidden_update.dimshuffle('x', 0)),\n",
    "                                   self.beta_hidden_update)\n",
    "            # Compute (1 - u_t)h_{t - 1} + u_t c_t\n",
    "            hid = (1 - updategate)*hid_previous + updategate*hidden_update\n",
    "            return hid\n",
    "\n",
    "        def step_masked(input_n, mask_n, hid_previous, *args):\n",
    "            hid = step(input_n, hid_previous, *args)\n",
    "\n",
    "            # Skip over any input with mask 0 by copying the previous\n",
    "            # hidden state; proceed normally for any input with mask 1.\n",
    "            hid = T.switch(mask_n, hid, hid_previous)\n",
    "\n",
    "            return hid\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask is given as (batch_size, seq_len). Because scan iterates\n",
    "            # over first dimension, we dimshuffle to (seq_len, batch_size) and\n",
    "            # add a broadcastable dimension\n",
    "            mask = mask.dimshuffle(1, 0, 'x')\n",
    "            sequences = [input, mask]\n",
    "            step_fun = step_masked\n",
    "        else:\n",
    "            sequences = [input]\n",
    "            step_fun = step\n",
    "\n",
    "        if not isinstance(self.hid_init, Layer):\n",
    "            # Dot against a 1s vector to repeat to shape (num_batch, num_units)\n",
    "            hid_init = T.dot(T.ones((num_batch, 1)), self.hid_init)\n",
    "\n",
    "        # The hidden-to-hidden weight matrix is always used in step\n",
    "        non_seqs = [W_hid_stacked, alpha_hid_stacked, beta_hid_stacked]\n",
    "        # When we aren't precomputing the input outside of scan, we need to\n",
    "        # provide the input weights and biases to the step function\n",
    "        if not self.precompute_input:\n",
    "            non_seqs += [W_in_stacked, b_stacked, alpha_in_stacked, beta_in_stacked]\n",
    "\n",
    "        if self.unroll_scan:\n",
    "            # Retrieve the dimensionality of the incoming layer\n",
    "            input_shape = self.input_shapes[0]\n",
    "            # Explicitly unroll the recurrence instead of using scan\n",
    "            hid_out = unroll_scan(\n",
    "                fn=step_fun,\n",
    "                sequences=sequences,\n",
    "                outputs_info=[hid_init],\n",
    "                go_backwards=self.backwards,\n",
    "                non_sequences=non_seqs,\n",
    "                n_steps=input_shape[1])[0]\n",
    "        else:\n",
    "            # Scan op iterates over first dimension of input and repeatedly\n",
    "            # applies the step function\n",
    "            hid_out = theano.scan(\n",
    "                fn=step_fun,\n",
    "                sequences=sequences,\n",
    "                go_backwards=self.backwards,\n",
    "                outputs_info=[hid_init],\n",
    "                non_sequences=non_seqs,\n",
    "                truncate_gradient=self.gradient_steps,\n",
    "                strict=False)[0]\n",
    "\n",
    "        return hid_out\n",
    "\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        # When it is requested that we only return the final sequence step,\n",
    "        # we need to slice it out immediately after scan is applied\n",
    "        hid_out = self.__gru_fun__(inputs, **kwargs)\n",
    "        if self.only_return_final:\n",
    "            hid_out = hid_out[-1]\n",
    "        else:\n",
    "            # dimshuffle back to (n_batch, n_time_steps, n_features))\n",
    "            hid_out = hid_out.dimshuffle(1, 0, 2)\n",
    "\n",
    "            # if scan is backward reverse the output\n",
    "            if self.backwards:\n",
    "                hid_out = hid_out[:, ::-1]\n",
    "\n",
    "        return hid_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEKCAYAAAAy4ujqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCHCBwwRPmCI8AFD/wNvStUiyku+oQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d1c112490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll use the load_data function from the mnist.py example\n",
    "from mnist import load_dataset\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_dataset()\n",
    "# Create a dataset dictionary for convenience\n",
    "dataset = {\n",
    "    'train': {'X': X_train, 'y': y_train},\n",
    "    'valid': {'X': X_valid, 'y': y_valid}}\n",
    "# Plot an example digit with its label\n",
    "plt.imshow(dataset['train']['X'][0][0], interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.title(\"Label: {}\".format(dataset['train']['y'][0]))\n",
    "plt.gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll determine the input shape from the first example from the training set.\n",
    "input_shape = dataset['train']['X'][0].shape\n",
    "l_in = lasagne.layers.InputLayer(\n",
    "    shape=(None, input_shape[0], input_shape[1], input_shape[2]))\n",
    "\n",
    "# Create the first convolutional layer\n",
    "l_conv1 = lasagne.layers.Conv2DLayer(\n",
    "    l_in,\n",
    "    # Here, we set the number of filters and their size.\n",
    "    num_filters=32, filter_size=(5, 5),\n",
    "    # lasagne.nonlinearities.rectify is the common ReLU nonlinearity\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    # Use He et. al.'s initialization\n",
    "    W=lasagne.init.HeNormal(gain='relu'))\n",
    "l_conv1_ln = lasagne.layers.normalization.layer_norm(l_conv1)\n",
    "# Other arguments: Convolution type (full, same, or valid) and stride\n",
    "\n",
    "# Here, we do 2x2 max pooling.  The max pooling layer also supports striding\n",
    "l_pool1 = lasagne.layers.MaxPool2DLayer(l_conv1_ln, pool_size=(2, 2))\n",
    "\n",
    "\n",
    "# The second convolution/pooling pair is the same as above.\n",
    "l_conv2 = lasagne.layers.Conv2DLayer(\n",
    "    l_pool1, num_filters=32, filter_size=(5, 5),\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.HeNormal(gain='relu'))\n",
    "l_conv2_ln = lasagne.layers.normalization.layer_norm(l_conv2)\n",
    "\n",
    "l_pool2 = lasagne.layers.MaxPool2DLayer(l_conv2_ln, pool_size=(2, 2))\n",
    "\n",
    "# p is the dropout probability\n",
    "#l_hidden1_dropout = lasagne.layers.DropoutLayer(l_hidden1, p=0.5)\n",
    "l_hidden1_reshape = lasagne.layers.ReshapeLayer(l_pool2, (l_in.input_var.shape[0], 32, 16))\n",
    "l_gru1 = lasagne.layers.LNGRULayer(l_hidden1_reshape, 30, grad_clipping=10, only_return_final=False)\n",
    "l_gru2 = lasagne.layers.LNGRULayer(l_hidden1_reshape, 30, grad_clipping=10, only_return_final=False, backwards=True)\n",
    "l_bgru = lasagne.layers.ConcatLayer([l_gru1, l_gru2], axis=2)\n",
    "\n",
    "l_hidden1 = lasagne.layers.DenseLayer(\n",
    "    l_bgru, num_units=256, \n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.HeNormal(gain='relu'))\n",
    "l_output = lasagne.layers.DenseLayer(\n",
    "    l_hidden1,\n",
    "    # The number of units in the softmas output layer is the number of classes.\n",
    "    num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_l_pool2 = theano.function([l_in.input_var],\n",
    "                             lasagne.layers.get_output(l_bgru, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m =dataset['train']['X'][0:100]\n",
    "n = test_l_pool2(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32, 60)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_output = T.ivector('true_output')\n",
    "\n",
    "# As mentioned above, when using dropout we should define different losses:\n",
    "# One for training, one for evaluation.  The training loss should apply dropout,\n",
    "# while the evaluation loss shouldn't.  This is controlled by setting get_output's deterministic kwarg.\n",
    "loss_train = T.mean(lasagne.objectives.categorical_crossentropy(\n",
    "        lasagne.layers.get_output(l_output, deterministic=False), true_output))\n",
    "loss_eval = T.mean(lasagne.objectives.categorical_crossentropy(\n",
    "        lasagne.layers.get_output(l_output, deterministic=True), true_output))\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_output, trainable=True)\n",
    "# Use ADADELTA for updates\n",
    "updates = lasagne.updates.adadelta(loss_train, all_params)\n",
    "train = theano.function([l_in.input_var, true_output], loss_train, updates=updates)\n",
    "\n",
    "# This is the function we'll use to compute the network's output given an input\n",
    "# (e.g., for computing accuracy).  Again, we don't want to apply dropout here\n",
    "# so we set the deterministic kwarg to True.\n",
    "get_output = theano.function([l_in.input_var],\n",
    "                             lasagne.layers.get_output(l_output, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation accuracy: 0.9794\n",
      "Epoch 2 validation accuracy: 0.9851\n",
      "Epoch 3 validation accuracy: 0.987\n",
      "Epoch 4 validation accuracy: 0.9886\n",
      "Epoch 5 validation accuracy: 0.9882\n",
      "Epoch 6 validation accuracy: 0.9885\n",
      "Epoch 7 validation accuracy: 0.9897\n",
      "Epoch 8 validation accuracy: 0.9897\n",
      "Epoch 9 validation accuracy: 0.9902\n",
      "Epoch 10 validation accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train it!  We'll chop the training data into mini-batches,\n",
    "# and compute the validation accuracy every epoch.\n",
    "BATCH_SIZE = 100\n",
    "N_EPOCHS = 10\n",
    "# Keep track of which batch we're training with\n",
    "batch_idx = 0\n",
    "# Keep track of which epoch we're on\n",
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "    # Extract the training data/label batch and update the parameters with it\n",
    "    train(dataset['train']['X'][batch_idx:batch_idx + BATCH_SIZE],\n",
    "          dataset['train']['y'][batch_idx:batch_idx + BATCH_SIZE])\n",
    "    batch_idx += BATCH_SIZE\n",
    "    # Once we've trained on the entire training set...\n",
    "    if batch_idx >= dataset['train']['X'].shape[0]:\n",
    "        # Reset the batch index\n",
    "        batch_idx = 0\n",
    "        # Update the number of epochs trained\n",
    "        epoch += 1\n",
    "        # Compute the network's output on the validation data\n",
    "        val_output = get_output(dataset['valid']['X'])\n",
    "        # The predicted class is just the index of the largest probability in the output\n",
    "        val_predictions = np.argmax(val_output, axis=1)\n",
    "        # The accuracy is the average number of correct predictions\n",
    "        accuracy = np.mean(val_predictions == dataset['valid']['y'])\n",
    "        print(\"Epoch {} validation accuracy: {}\".format(epoch, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
